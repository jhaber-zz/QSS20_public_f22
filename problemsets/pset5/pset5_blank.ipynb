{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 5: APIs, SQL, and supervised machine learning\n",
    "\n",
    "**Total points (without extra credit)**: 56 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep work: obtain an API key for the US Census (the American Community Survey)\n",
    "\n",
    "- Obtain a census API key from here: https://api.census.gov/data/key_signup.html \n",
    "- Place it in [the same credentials yaml file on GitHub that contains the SQL database access information](https://github.com/jhaber-zz/QSS20_public/blob/main/activities/11_db_cred.yaml) (password, host, etc.) \n",
    "    - Name the combined credentials file something appropriate (feel free to get creative)\n",
    "    - Change the database name from `sentencing` to `math_gencompare`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import plotnine \n",
    "from plotnine import *\n",
    "import yaml\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## note: you may need to install these using !pip install\n",
    "#!pip install census\n",
    "#!pip install us\n",
    "import census\n",
    "from census import Census\n",
    "import us\n",
    "from us import states\n",
    "import mysql.connector\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "def load_creds(path: str):\n",
    "    with open(path, 'r') as stream:\n",
    "        try:\n",
    "            creds = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return(creds)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a wrapper function to pull data from the NAEP API (12 points)\n",
    "\n",
    "In the class activity here: https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/08_apis_partI_solutions.ipynb \n",
    "\n",
    "We practiced pulling from the API for the National Assessment of Educational Progress (NAEP), \"America's report card\" of test scores. We pulled a small amount of data at the national level (writing scores by gender) using a query where the parameters were hardcoded.\n",
    "    \n",
    "In this problem, we'll practice pulling a larger set of data and writing a wrapper function.\n",
    "    \n",
    "As a reminder, the documentation is here: https://www.nationsreportcard.gov/api_documentation.aspx\n",
    "\n",
    "The base link is: https://www.nationsreportcard.gov/Dataservice/GetAdhocData.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Write a query to pull 8th-grade mathematics scores in 2015 from the state of California (CA) by gender (1 point)\n",
    "\n",
    "- Subject: mathematics \n",
    "- Subscale: MRPCM composite scale \n",
    "- Grade: 8\n",
    "- Year: 2015\n",
    "- grouping variable: GENDER \n",
    "- Jurisdiction: CA \n",
    "- stattype = MN (for mean)\n",
    "\n",
    "Print the output in dataframe format and briefly interpret; what do scores look like between the genders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Write a query to pull 8th-grade mathematics scores in 2013, 2015, 2017, and 2019 from California by gender (1 point)\n",
    "\n",
    "Same as 1.1 but pull the years 2013, 2015, 2017, and 2019 (search documentation for how to combine) in one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create a line plot to show variation in the scores across years (2 points)\n",
    "\n",
    "Using the results from 1.2, create a plot where the x axis has the year and the y axis is the math scores (`value` in dataframe), and there are separate lines/colors for male versus female students (`varValueLabel` in dataframe)\n",
    "\n",
    "Start the limits of the y axis at 270 and add informative labels. Be sure your x-axis is ticked on odd years, because NAEP scores skip even years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reproduce the queries from 1.1 and 1.2 using a user-defined function (4 points)\n",
    "\n",
    "Create a function, `construct_naep_query` that takes in two arguments:\n",
    "\n",
    "- year: this should be a list with all years (so if one year, single element list; if multiple years, list with those years)\n",
    "- place: this should be a string with the name of the state or jurisdiction to pull \n",
    "    \n",
    "Have the function return the query and make sure it's identical to the queries you wrote for 1.1 and 1.2 (can use assert or other checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to execute function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Write and execute user-defined function that takes in a query and returns a pandas dataframe with the content of the response (4 points)\n",
    "\n",
    "- Write a user-defined function (`process_naep_query`) that takes in the CA-specific NAEP query as a string, calls the API, and transforms the response into a pandas dataframe. Have the function return that pandas dataframe\n",
    "\n",
    "- Make sure the function is flexible enough to handle queries that return an error; for queries that return an error, have the function return the string \"Data not found; check your query\" (see [API part 1 solutions code](https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/08_apis_partI_solutions.ipynb) for an example of `try:`/`except:`)\n",
    "\n",
    "- Execute the function on the query that pulls 2013-2019 data (either from handwriting the query or the result in 1.4)\n",
    "\n",
    "- Print the resulting dataframe\n",
    "\n",
    "- Then execute the function on a query that pulls a state that doesn't exist (call this state ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explore data using SQL queries (18 points)\n",
    "\n",
    "In the previous example, you worked with the data in a flat file and manipulated it using pandas. Here, we're going to practice running queries to do some calculations using SQL --- in the case of our data, this is a bit overkill since the data are small but it is practice for larger datasets.\n",
    "\n",
    "**Resources for these problems**: \n",
    "\n",
    "- Example code: https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/11_SQL_examplecode.ipynb\n",
    "- Activity solutions: https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/11_SQL_activity_solutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load database credentials and establish a connection (1 point)\n",
    "\n",
    "Load a credentials file that contains the credentials you'll need for this and the next problem:\n",
    "\n",
    "- The credentials for our class database\n",
    "- The credentials for the Census API (see instructions above)\n",
    "\n",
    "Note: to establish the SQL connection, you need to be on `eduroam` (near campus) or the Dartmouth's GlobalProtect `VPN`  ([installation instructions here](https://services.dartmouth.edu/TDClient/1806/Portal/KB/?CategoryID=17668))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = load_creds(\"PATH TO YOUR CREDS FILE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run a query to select all columns and the first 5 rows of the math_gencompare database to explore structure (2 points)\n",
    "\n",
    "Read the results in as a pandas dataframe and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Find the (1) number of rows in the database, (2) number of distinct states,  (3) number of distinct years (3 points)\n",
    "\n",
    "Interpret the results - how do you think the data is structured in terms of states and years (eg long format where each state repeated; wide format)?\n",
    "\n",
    "**Hint**: rather than using count `(*)` for the latter two, think about the `distinct` command in combination with `count`: https://www.w3resource.com/mysql/aggregate-functions-and-grouping/aggregate-functions-and-grouping-count-with-distinct.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Construct a new variable, `is_male_higher` that takes the value of 1 if the math scores of males exceed that of females in that state and year (each row) (2 points)\n",
    "\n",
    "Read in the results, print the head, and find the mean across all rows (the percentage of state-years where male students have higher scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 group by year and find the percentage of states where male scores are higher than females (4 points)\n",
    "\n",
    "**A.** Write a query that (1) groups by year and (2) finds the percentage of states that have higher scores for males than females in this year \n",
    "\n",
    "**B.** Print the resulting dataframe and interpret the results \n",
    "\n",
    "**Hint:** To compare male and female scores, consider logical operators (e.g., `<`, `>`, `=`) and simple aggregation (e.g., `avg()` to get mean) or using a subquery to construct the indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 group by state and find the percentage of years where male scores higher than females\n",
    "\n",
    "**A.** Write a query that (1) groups by state and (2) finds the percentage of years that have higher scores for males than females in that state\n",
    "\n",
    "**B.** Plot the results ordering the states from males higher all 4 years to males higher none of the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Use a subquery to create an indicator and group by that indicator (6 points)\n",
    "\n",
    "The following states were the first 6 to expand the right to vote to women before the uniform federal expansion in 1920\n",
    "\n",
    "- Wyoming 1890\n",
    "- Colorado 1893\n",
    "- Utah 1896\n",
    "- Idaho 1896\n",
    "- Washington 1910\n",
    "- California 1911\n",
    "\n",
    "**A.** Create an indicator `is_early_vote` for whether a state is in that list or not; do so without typing the state names inside the string and instead collapsing the list of states we provide and using something like `format`. Hint on how to combine the state names while preserving the quotes around each: https://stackoverflow.com/questions/12007686/join-a-list-of-strings-in-python-and-wrap-each-string-in-quotation-marks \n",
    "\n",
    "**B.** Then, group by the `is_early_vote` indicator and `year` and find the percencentage of states in each group where males had higher scores than females \n",
    "\n",
    "**C.** Print the resulting dataframe and interpret. Does early expansion of voting seem to be correlated with girls scoring better on the math tests a century later?\n",
    "\n",
    "**Hint:** in order to group by the indicator in step b, you may need to use a subquery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of states we provide\n",
    "list_suffrage = [\"Wyoming\", \"Colorado\", \"Utah\", \"Idaho\", \"Washington\", \n",
    "                \"California\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pull state-level attributes using Census API (8 points)\n",
    "\n",
    "You want to explain the variation you see across states in gender gaps in test scores by looking at demographics of the state population. To do so, we'll pull demographics from the American Community Survey (ACS), a US Census data product discussed more here: https://en.wikipedia.org/wiki/American_Community_Survey\n",
    "\n",
    "## 3.1 Initialize Census API connection using API key (0 points)\n",
    "\n",
    "- Load the credentials yaml file with the Census API key\n",
    "- Initialize the Census API connection using API key and the `census` package\n",
    "    - Documentation here for the `census` package on establishing an API connection: https://github.com/datamade/census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run this function (feeding it your api connection) to get a list of variables to pull (0 points)\n",
    "\n",
    "Feed the connection to the API you created in previous step (if you print type it's a census.core.Census class) to the `your_connection` argument in the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep in blank\n",
    "to_pull = ['RATIO OF INCOME TO POVERTY LEVEL OF FAMILIES IN THE PAST 12 MONTHS',\n",
    "          'ALLOCATION OF HOUSEHOLD INCOME IN THE PAST 12 MONTHS - PERCENT OF INCOME ALLOCATED',\n",
    "          'MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2018 INFLATION-ADJUSTED DOLLARS)',\n",
    "          'EDUCATIONAL ATTAINMENT FOR THE POPULATION 25 YEARS AND OVER',\n",
    "          'HOUSEHOLD TYPE (INCLUDING LIVING ALONE) BY RELATIONSHIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep in blank\n",
    "def get_acs_varnames(your_connection):\n",
    "    \n",
    "    ## get tables for the acs 5-year estimates\n",
    "    all_tables = pd.DataFrame(your_connection.acs5.tables(year = 2018))\n",
    "    \n",
    "    ## specify the ones to pull\n",
    "    info_topull = all_tables[all_tables.description.isin(to_pull)].copy()\n",
    "    \n",
    "    ## use raw api to get varnames within those tables\n",
    "    all_vars = [pd.DataFrame(requests.get(one_table).json()['variables']).T\n",
    "                for one_table in info_topull.variables]\n",
    "    all_vars_df = pd.concat(all_vars)\n",
    "    all_vars_df['varname'] = all_vars_df.index\n",
    "    \n",
    "    ## subset to relevant\n",
    "    all_vars_df_subset = all_vars_df[['varname', 'group', 'label',\n",
    "                                    'concept']].copy()\n",
    "    all_vars_df_est = all_vars_df_subset[all_vars_df.varname.str.contains(\"E$\", \n",
    "                                        regex = True)].copy()\n",
    "    return(all_vars_df_est)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: replace the your_connection with your \n",
    "## connection to the Census API\n",
    "acs_cols = get_acs_varnames(your_connection)\n",
    "acs_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pull the variables for all 50 states (8 points)\n",
    "\n",
    "**A.** Use list comprehension to pull these variables for every state (each FIPS code) using the `acs5.state` method for the year 2013.\n",
    "- **Hint 1:** See the documentation for how to feed it variables to pull (requires a tuple); the documentation shows an example state--`MD`. You can find the other `FIPS` Ccodes for states in the `states` object in `us` package you loaded above): https://github.com/datamade/census/blob/70e2c08710c1e10e5bc2054b78613fa8794d4765/README.rst\n",
    "- **Hint 2:** You can feed the tuple of ACS variable names to the function you'll use to pull Census/ACS data; you should NOT need to iterate over individual vars when pulling.\n",
    "- **Note**: this takes ~1-2 minutes to run on our machine\n",
    "\n",
    "**B.** Transform the result (which is a list of json) into a list of dataframes. Then concatenate and melt (on state) into one dataframe\n",
    "\n",
    "**C.** Merge with the `all_states_fips` df, then merge that with `acs_cols` from 3.2 (on varname and variable) to know both which states the variables correspond to and the more informative variable names \n",
    "\n",
    "Call the final output `acs_df_forperc` so you can run the next code we provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this code- keep in blank\n",
    "## NOTE: you need to have the us package\n",
    "## installed and imported\n",
    "cols_pull = tuple(acs_cols.varname) # tuple form of the acs variable names to pull\n",
    "all_states = states.STATES\n",
    "all_states_fips = pd.DataFrame({'FIPS':\n",
    "                    [one_state.fips for one_state in all_states],\n",
    "                    'state': [one_state.name for one_state in all_states],\n",
    "                'abbrev': [one_state.abbr for one_state in all_states]})\n",
    "all_states_fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here for remainder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 run code to transform counts into percentages (0 points)\n",
    "\n",
    "- Run the following code to transform the ACS counts in `acs_df_forperc` into percentages\n",
    "\n",
    "Note: You may see a warning from the str.split step; feel free to ignore it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_percentages(df, name_estimatecol = 'estimate'):\n",
    "    \n",
    "    ## remove cols that dont need percentages\n",
    "    df_forperc = df[~df.perc_NA].copy()\n",
    "    \n",
    "    ## sort by \n",
    "    \n",
    "    ## group by location and variable prefix \n",
    "    group_co_tract_varg = df_forperc.groupby(['FIPS', 'variable_prefix'])\n",
    "    \n",
    "    ## iterate over groups\n",
    "    df_longperc = []\n",
    "    for group, data_raw in group_co_tract_varg:\n",
    "        prefix = data_raw.variable_prefix.iloc[0]\n",
    "        FIPS = data_raw.FIPS.iloc[0]\n",
    "        row_list_group = []\n",
    "        data = data_raw.sort_values(by = 'variable_suffix')\n",
    "        for i in range(1, data.shape[0]):\n",
    "            numerator = data[name_estimatecol].iloc[i]\n",
    "            denominator = float(data[name_estimatecol].iloc[0])\n",
    "            if denominator == 0:\n",
    "                denominator = np.nan\n",
    "            if denominator != 0:\n",
    "                percentage = numerator / denominator\n",
    "                row = [prefix, FIPS]\n",
    "                row = row + [data.variable_suffix.iloc[i], percentage]\n",
    "                row_list_group.append(row)\n",
    "        df_longperc.append(pd.DataFrame(row_list_group))\n",
    "    percentages_all_groups = pd.concat(df_longperc)\n",
    "    percentages_all_groups.columns = ['variable_prefix',\"FIPS\",\n",
    "                                  'variable_suffix', 'percentage']\n",
    "    percentages_all_groups['percentage'] = percentages_all_groups.percentage.astype(float)\n",
    "    return(percentages_all_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames_percnotrelevant = [\"B19013_001E\"]\n",
    "\n",
    "## create prefix and suffix columns\n",
    "acs_df_forperc['variable_prefix'], acs_df_forperc['variable_suffix'] = \\\n",
    "                                acs_df_forperc['varname'].str.split('_', 1).str\n",
    "acs_df_forperc['perc_NA'] = np.where(acs_df_forperc.varname.isin(varnames_percnotrelevant),\n",
    "                                  True, False)\n",
    "acs_df_forperc = acs_df_forperc[acs_df_forperc.variable != \"GEO_ID\"].copy()\n",
    "\n",
    "perc_long = create_percentages(acs_df_forperc, 'value').sort_values(by = 'variable_prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_long_wnames = pd.merge(perc_long,\n",
    "                           acs_df_forperc,\n",
    "                           on = ['variable_prefix', 'variable_suffix', 'FIPS'],\n",
    "                           how = \"right\")\n",
    "perc_long_wnames['value'] = perc_long_wnames.value.astype(float)\n",
    "\n",
    "perc_long_wnames['percentage'] = np.where(perc_long_wnames.perc_NA,\n",
    "                                         perc_long_wnames.value,\n",
    "                                         perc_long_wnames.percentage)\n",
    "perc_long_wnames['varname_words'] = \"acspredict_\" + perc_long_wnames.concept.str.replace(\"\\s+|\\(|\\)\", \"_\", \n",
    "                                    regex = True).str.lower() + \\\n",
    "                            perc_long_wnames.label.str.replace(\"\\.|\\!|\\,|\\(|\\)|\\-\", \n",
    "                                    \"\", regex = True).str.lower() \n",
    "\n",
    "perc_long_wnames_final = perc_long_wnames[['FIPS', 'percentage', \n",
    "                                          'varname_words']].copy()\n",
    "\n",
    "\n",
    "perc_wide = pd.pivot_table(perc_long_wnames_final, \n",
    "                           index = 'FIPS',\n",
    "                          columns='varname_words',\n",
    "                            values='percentage').reset_index()\n",
    "\n",
    "## merge state info back on\n",
    "perc_wide_wstate = pd.merge(perc_wide,\n",
    "                           all_states_fips,\n",
    "                           on = \"FIPS\",\n",
    "                           how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_wide_wstate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 extra credit (2 points)\n",
    "\n",
    "Use list comprehension and NAEP query creation/process results functions you created above to iterate over state abbreviations in `all_states_fips` and pull the same test score gap information\n",
    "\n",
    "If skipping, you'll read in pkl at next step\n",
    "\n",
    "**Note**: this took 2 mins to run on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore variation in math score disparities and trends (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Read in the `acs_wmath.pkl` file (csv is backup) (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create a visualization where one axis is the state; the other axis is the male 2013 math scores - the female 2013 math scores (gender disparity) (2 points)\n",
    "\n",
    "\n",
    "You have free rein over additional details but make sure it is informative over what direction of disparity positive versus negative values mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Examine gender disparity in relation to household income (6 points)\n",
    "\n",
    "**A.** Construct an indicator variable for the state having better performance of males in 2013 than females\n",
    "\n",
    "**B.** First plot a smoothed scatterplot of estimated median household income from the acs data (we provide varname below) vs `math_male_2013`. Then do a second smoothed scatterplot for median household income vs `math_female_2013`.\n",
    "\n",
    "**C.** \n",
    "Then use the `np.corrcoef` command (three separate times) to examine the bivariate correlation of\n",
    "- male performance\n",
    "- female performance\n",
    "- the indicator variable from **A** \n",
    "\n",
    "with median household income (`acspredict_median_household_income_in_the_past_12_months__in_2018_inflation-adjusted_dollars_estimatemedian household income in the past 12 months in 2018 inflationadjusted dollars`)\n",
    "\n",
    "Documentation: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\n",
    "\n",
    "**D.** Interpret the correlations - in states with higher median household income (MHI), do \n",
    "   - boys tend to perform better than boys in states with lower MHI?\n",
    "   - girls tend to perform better than girls in states with lower MHI?\n",
    "   - boys tend to outperform girls more than they do in states with lower MHI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Predicting disparities (10 points)\n",
    "\n",
    "**A.** Read in the raw `acs_wmath` data again (this loses the variables you created above)\n",
    "\n",
    "**B.** Construct a binary indicator variable for male score > female score  for each year - for full credit, do so without repeating the difference code for each of the four years: name these according to following convention: `outcome_male_higher_female_year` where year is 2013, 2015, 2017, or 2019 (e.g., 2013: `outcome_male_higher_female_2013`). After this, remove the raw math scores as columns in the data (so filter out any column with the word math)\n",
    "\n",
    "**C.** Melt the data (`acs_wmath`) to long where instead of wide years, years are repeated within state; the ACS vars will also be repeated since we only pulled one year. In other words, reshape the data from \"wide format\", where each state is a row and we have separate columns for each year, to \"long format\", where states are repeated four times: once for each year in the data (2013, 2015, 2017, 2019)\n",
    "\n",
    "**D.** Split into train-test split at state level (so all years in same state -> either all in train or all test). Randomize 35 states to train; 15 states in test. \n",
    "\n",
    "**E.** Normalize the features to mean 0, variance 1 and estimate a decision tree with a max depth of 5. \n",
    "\n",
    "- **Hint:** The ML literature recommends using the training set scaler to transform the test set, rather than using a unique scaler to initialize each one. The reasons are discussed here: https://stats.stackexchange.com/questions/495357/why-do-we-normalize-test-data-on-the-parameters-of-the-training-data\n",
    "\n",
    "**F.** Interpret the feature importances\n",
    "\n",
    "**G.** Evaluate the precision and recall of that model in the test set states without using the `score`, `precision`, or `recall` functions in sklearn. Briefly interpret: compared to our class example (a high-dimensional feature matrix of yelp reviews with ~15000 observations), why do you think our models perform worse for this set of data/predictors?\n",
    "\n",
    "**Resources:** \n",
    "\n",
    "- Lecture slides on supervised ML: https://github.com/jhaber-zz/QSS20_public/blob/main/slides/11_qss20_f22_supervisedML.pdf\n",
    "\n",
    "- Intro activity for supervised ML: https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/09_ML_intro_activity_solutions.ipynb\n",
    "\n",
    "- Activity on Decision Tree & model evaluation in supervised ML: https://github.com/jhaber-zz/QSS20_public/blob/main/activities/solutions/09_ML_optimization_activity_solutions.ipynb\n",
    "\n",
    "- Feature normalization: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "- Definition of precision and recall: https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. your code here to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. your code here to construct binary indicators for male higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. your code here to melt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. your code here for train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. your code here to normalize features and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. your code here to interpret feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G. your code here to evaluate model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
