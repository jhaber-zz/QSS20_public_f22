{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling broadly with `Scrapy`\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Crawling broadly with `Scrapy`](#scrapy)\n",
    "    * [A simple (narrow) spider](#simple)\n",
    "    * [Link extraction in a (broad) spider](#linkextraction)\n",
    "\n",
    "**__________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `Scrapy` <a id='scrapy'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple (narrow) spider <a id='simple'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "class SimpleSpider(scrapy.Spider):\n",
    "    name = \"simple\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ scrapy crawl simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Modify and run the spider script above to scrape this short list of `start_urls`: \n",
    "```python\n",
    "['http://www.baylessk12.org/', 'https://crcc.doniphanr1.k12.mo.us/', 'https://www.hazelwoodschools.org/southeastmiddle']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Inspect [quotes.toscrape.com](quotes.toscrape.com) for the selectors associated with quotes. Use this information to display the text of one of the quotes in the scrapy shell. <br>\n",
    "**Hint 1:** If you need help getting a better sense of website structure, use the HTML tree below (under \"Extracting quotes and authors\") as a visual guide.<br>\n",
    "**Hint 2:** You can subset within selectors by using periods and spaces. For instance, the following produces a SelectorList for the class2 of each type2 within the class1 of each type1:\n",
    "```shell\n",
    "response.css('type1.class1 type2.class2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "```shell\n",
    "response.css(div.quote span.text::text).get()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link extraction in a (broad) spider <a id='linkextraction'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Adapt the `CrawlSpider` in `broad.py` to scrape the text, author, and tag for each quote across all the page on `http://quotes.toscrape.com`. Assign the `text`, `author`, and `tags` fields to Items, then yield the Items. Edit the spider script first, then run it via your terminal, then check the output to make sure.\n",
    "\n",
    "```python\n",
    "# solution\n",
    "\n",
    "from schools.items import SchoolsItem\n",
    "\n",
    "class BroadSpider(CrawlSpider):\n",
    "    name = 'broad'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['quotes.toscrape.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(), \n",
    "             callback='parse_item', \n",
    "             follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            item = SchoolsItem() # initialize\n",
    "            \n",
    "            item['text'] = quote.css('span.text::text').get(),\n",
    "            item['author'] = quote.css('small.author::text').get(),\n",
    "            item['tags'] = quote.css('div.tags a.tag::text').getall(),\n",
    "    \n",
    "            yield item\n",
    "```\n",
    "\n",
    "Call it like so:\n",
    "```shell\n",
    "scrapy crawl broad -o quotes_broad.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use what you learned above about removing tags with an exclusion list to rewrite the `parse_item()` function in the `BroadSpider()` so that it doesn't depend on website structure (HTML, CSS, XPath, etc.). In other words, write a truly broad crawler that only returns text.\n",
    "\n",
    "Make sure to clean up the spacing: convert multiple newlines into a single one or a space, depending on the output format you want. \n",
    "\n",
    "_Hint:_ Check your output--is it missing anything important? Consider removing specific tags from the exclusion list. \n",
    "\n",
    "\n",
    "```python\n",
    "# solution\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "...\n",
    "def parse_item(self, response):\n",
    "    item = SchoolsItem()\n",
    "        \n",
    "    # Load HTML into BeautifulSoup, extract text\n",
    "    soup = BeautifulSoup(response.body, 'html.parser') # default parser, 'lxml' is faster\n",
    "        \n",
    "    # Remove non-visible tags from soup\n",
    "    [tag.decompose() for tag in soup(tags_exclusions)]\n",
    "        \n",
    "    # Extract text, remove <p> tags\n",
    "    visible_text = soup.get_text(strip = False) # get text from each chunk, leave unicode spacing (e.g., `\\xa0`) for now to avoid globbing words\n",
    "        \n",
    "    # Replace any consecutive linebreaks with a single newline\n",
    "    visible_text = re.sub(r\"[ *\\n *]+\", \" \", visible_text).strip() # remove trailing whitespaces too\n",
    "        \n",
    "    item['text'] = visible_text # assign text to item\n",
    "    item['url'] = response.url # assign url too\n",
    "        \n",
    "    return item\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
