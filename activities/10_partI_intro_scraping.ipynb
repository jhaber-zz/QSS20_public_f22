{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web-scraping\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Making `Requests`](#request)\n",
    "* [Parsing HTML](#parsing)\n",
    "    * [Pretty parsing with `BeautifulSoup`](#BS)\n",
    "    * [Getting human-readable text](#readable)\n",
    "* [URL collection with automated Google search](#URLs)\n",
    "    * [Scraping school URLs](#school_URLs)\n",
    "    * [Scraping URLs using an exclusion list](#exclusionlist)\n",
    "\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing\n",
    "from bs4 import BeautifulSoup # essential package for parsing in Python\n",
    "import requests # for web requests\n",
    "\n",
    "# for automated URL collection\n",
    "!pip install google # needs running only once\n",
    "from googlesearch import search # automated Google search package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making `Requests` <a id='request'></a>\n",
    "\n",
    "The first step in web-scraping is getting the HTML of the website we want to scrape. The [requests](http://docs.python-requests.org/en/master/) library is the easiest way to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Kickapoo_people'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like everything worked! Let's see our beautiful HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that's weird. Doesn't look like HTML to me.\n",
    "\n",
    "What the `requests.get` function returned (and the thing in our `response` variable) was a Response object. It itself isn't the HTML that we wanted, but rather a collection of metadata about the request/response interaction between your computer and the Wikipedia server.\n",
    "\n",
    "For example, it knows whether the response was successful or not (`response.ok`), how long the whole interaction took (`response.elapsed`), what time the request took place (`response.headers['Date']`) and a whole bunch of other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what we really care about is the HTML content. We can get that from the `Response` object with `response.text`. What we get back is a string of HTML, exactly the contents of the HTML file at the URL that we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.text\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML for [this claim review by fact checking site PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/). \n",
    "Print out the first 1000 characters and compare it to the HTML you see when you view the source HTML in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML <a id='parsing'></a>\n",
    "\n",
    "After getting HTML (e.g., with `Requests`), the second step in web-scraping is parsing HTML. This is where things can get a little tricky.\n",
    "\n",
    "Let's start by looking more closely at HTML. Use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML of [the page listing all Fall 2022 Sociology courses at Georgetown University](https://myaccess.georgetown.edu/pls/bninbp/bwckgens.p_proc_term_date?p_term=202230&p_calling_proc=bwckschd.p_disp_dyn_sched#_ga=2.176635429.1632560928.1616506374-1636514659.1616506374) in your browser (select \"Sociology\" from the list then click \"Class Search\"), and find the part of the HTML where the course headings are listed. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a course you see on the webpage.\n",
    "\n",
    "You should see listings like these ones:\n",
    "\n",
    "```html\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th class=\"ddtitle\" scope=\"colgroup\">\n",
    "      <a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38298\">Introduction to Sociology - 38298 - SOCI 001 - 01</a>\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"ddtitle\" scope=\"colgroup\">\n",
    "      <a href=\"/pls/bninbp/bwckschd.p_disp_detail_sched?term_in=202130&amp;crn_in=38299\">Introduction to Sociology - 38299 - SOCI 001 - 02</a>\n",
    "    </th>\n",
    "  </tr>\n",
    "  ...\n",
    "```\n",
    "\n",
    "This is HTML. HTML uses \"tags\", code that surrounds the raw text which indicates the structure of the content. The tags are enclosed in `<` and `>` symbols. The `<li>` says \"this is a new thing in a list\", and `</li>` says \"that's the end of that new thing in the list\". Similarly, the `<a ...>` and the `</a>` say, \"everything between us is a hyperlink\". And likewise, the `<tr>`and the `</tr>` enclose a table row, while the `<th>`and the `</th>` enclose row cells that contain column headers.\n",
    "\n",
    "In this HTML file, each course title is listed with `<th>...</th>` and is also linked to its own page using `<a>...</a>`. In our browser, if we click on the name of the department, it takes us to detailed information for that class, including Registration Availability. You'll see inside the `<a>` bit, there's a `href=...`. That tells us the (relative) location of the page it's linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty parsing with `BeautifulSoup` <a id='BS'></a>\n",
    "\n",
    "Armed with this knowledge of HTML, let's try getting the HTML and parsing the fact checking page we saw in part 1. We will use `requests` to get the HTML and its text, then `BeautifulSoup` to parse the result. (Check out [the `BeautifulSoup` docs](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for lots of tips and tricks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URL to scrape\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "# Scrape HTML\n",
    "html = requests.get(url)\n",
    "\n",
    "# Convert HTML into soup object\n",
    "soup = BeautifulSoup(html.text) # use default 'html.parser' ('lxml' is faster though)\n",
    "\n",
    "# See pretty formatting in soup object\n",
    "print(soup.prettify()[:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty! Much more so than the plain ol' `requests.get().text` block we saw in part 1. But this is just the beginning of what `BeautifulSoup` can do. It can also find specific tags, like paragraphs (via `<p>`), headers (via `h1`, `h2`, etc.), and hyperlinks (via `<a>` and their `href` elements).\n",
    "\n",
    "In most cases, the `<p>` tag is the most useful for extracting readable text from a webpage. Let's get the first 10 paragraph tags from this claim review page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Find all the links in the above claim review page using the `<a>` tags and their `href` elements. Print every 10th link. What do you notice about where these links point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes these tags aren't very useful--in fact, they can get in the way of extracting only visible or human-readable text from the HTML. This too can be accomplished with `BeautifulSoup`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting human-readable text <a id='readable'></a>\n",
    "\n",
    "Occasionally we want to learn about websites via their tags: What the headers say, which paragraph comes first, where the links or images are, etc. Other times tags (such as scripts or styles) only introduce extraneous characters and nonsense words, and we want to ignore the tags themselves or even the text they enclose. \n",
    "\n",
    "The simplest way to do this is with the `get_text()` method in `BeautifulSoup`, which returns all the text in a document or beneath a tag, as a single Unicode string. You might have noticed that the `<p>` tags got in the way in our above example. Let's try that again and this time, we will remove the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph.get_text().strip()) # extract text and strip trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also easy to call the first element of the soup object matching a given tag, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p.get_text() # Get text of first paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is `extract()`, which can be used to surgically remove a tag or string from the soup tree, storing it for safe keeping. Let's extract the first 5 links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = [] # initialize list of extracted links\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first five <a> tags\n",
    "    extracted.append(link.extract()) # extract the link\n",
    "    \n",
    "print('Extracted links:')\n",
    "print(extracted)\n",
    "print()\n",
    "\n",
    "# What are the first five links now that the previous five were removed? \n",
    "print('Remaining links:')\n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't want to keep the tag at all? In this case, we would use `decompose()`, which obliterates a useless tag (and frees up memory). Unlike with `extract()`, with `decompose()` you don't need to assign the junk tag to anything to clear it--the method does this automatically. \n",
    "\n",
    "Let's try the above code again, this time with `decompose()` and `get_text()` to clean up the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first five <a> tags\n",
    "    link.decompose() # obliterate this link\n",
    "    \n",
    "# What are the first five links now the the previous five were removed? \n",
    "print('Remaining links:')\n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link.get_text().strip()) # get text and clean spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all websites use the `<p>` tag to indicate the important, human-readable text. Sometimes we need to approach HTML parsing from the other end: By finding and removing all non-informative tags. Let's use `BeautifulSoup` to build such a method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use `decompose()` to remove from the soup all tags showing anything other than human-readable text. Below is a list of such junk tags to use as an exclusion list. \n",
    "\n",
    "```\n",
    "\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "\"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "\"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"\n",
    "```\n",
    "\n",
    "_Hint:_ Iterate over these tags to identify each one in the soup and remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that word boundaries get clobbered when you call `get_text()`. This is because the default setting for this method is `strip=True`, which tells `BeautifulSoup` to strip whitespaces (of any kind) from the beginning and end of each bit of text. Using `strip=False` leads to lots of extra whitespaces--usually, newlines--which requires some regular expressions to clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Using the above tags exclusion list and `decompose()` as before, this time use the `strip=False` parameter when calling `get_text()` to avoid combining words across whitespace boundaries. Instead, use regular expressions to clean up extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challenge\n",
    "\n",
    "You might have noticed that when we scraped HTML above from [this claim review by PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/), we got headers and tags like this:\n",
    "```html\n",
    "<p>Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.</p>\n",
    "<p>\n",
    "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
    "</p>\n",
    "<p class=\"c-image__caption-inner copy-xs\">\n",
    "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
    "</p>\n",
    "```\n",
    "Use what you now know about identifying HTML, removing tags, and cleaning spacing to scrape a clean explanation from the body of this article. \n",
    "\n",
    "_Hint:_ Use your browser to inspect this website's HTML and identify any unique types and/or classes that enclose the explanation (and nothing else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output from this focused, site-specific scraping approach with that from the exclusion list method above. <br/>\n",
    "**Which method gives the cleaner output? Which method is more extensible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL collection with automated Google search<a id='URLs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to crawl and/or scrape an online community of websites, there's a good chance may find yourself needing to collect their URLs. If you're lucky, you have comprehensive metadata describing these entities, something like their name and physical address. Your next step in this scenario would be to automate a Google search to collect the best URL matching each entity. \n",
    "\n",
    "How can you scrape URLs from Google? There are two fairly easy ways.\n",
    "\n",
    "First, the **Google Places API**, which is the best option to do this at scale. You would need to apply for an API key from Google: go to the [Google cloud console](https://console.cloud.google.com/), create a project, and request an API key for each service you want to use. Approval may take a few days, but once done there is a [handy Python wrapper](https://github.com/slimkrazy/python-google-places) to make this easy to use in Python. See [Google Web Services](https://developers.google.com/places/web-service/) for general documentation and [Google Developers](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    "The second option is **automated Google search**, which is not nearly as reliable and may get you blocked if used repeatedly. This method tends to get lots of false positives and third-party website aggregators (e.g., yellowpages.com, trulia.com), so using an exclusion list to manually filter results is a good idea. Check out [the source code](https://github.com/MarioVilas/googlesearch) and [documentation](https://python-googlesearch.readthedocs.io/en/latest/). _Thanks Mario Vilas for this package!_\n",
    "\n",
    "Because this second option is free and has no waiting period to use, we will practice using this in a nice way. In case you want to pursue further the first option, at the bottom of this notebook there is template code for running the Google Places API.\n",
    "\n",
    "_Note_: Remember what I said about following the Terms of Service for APIs? You might find real gems in there--like this extract from the [Google Maps Platform Terms of Service](https://developers.google.com/terms/) that prohibits scraping data you intend to store:\n",
    "\n",
    "```\n",
    "3.2.3 Restrictions Against Misusing the Services.\n",
    "\n",
    "(a)  No Scraping. Customer will not export, extract, or otherwise scrape Google Maps Content for use outside the Services. For example, Customer will not: (i) pre-fetch, index, store, reshare, or rehost Google Maps Content outside the services; (ii) bulk download Google Maps tiles, Street View images, geocodes, directions, distance matrix results, roads information, places information, elevation values, and time zone details; (iii) copy and save business names, addresses, or user reviews; or (iv) use Google Maps Content with text-to-speech services.\n",
    "```\n",
    "\n",
    "Keep this in mind should you consider using the Google Places API for URL scraping (as my template code below does): The same terms apply, so be nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping school URLs<a id='school_URLs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, let's start by searching for the best URL for a charter school in Washington, D.C. Assume we have the name and address of the school.\n",
    "\n",
    "To prevent overwhelming Google search with rapid requests--and likely getting our IP address blocked by Google as a result--let's search only for the first 10 results and include a five-second pause in between each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata for a single entity: a DC charter school\n",
    "school_name = 'Capital City Public Charter School'\n",
    "school_address = '100 Peabody Street NW, Washington, DC 20011'\n",
    "\n",
    "# Search for first 10 Google results using joined metadata, show each one\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=10, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty strong result: the first six matches share the domain of https://www.ccpcs.org/, so this is probably the best match. We identified a URL without even visiting any websites!\n",
    "\n",
    "Notice that results 7-10 are about the right school, but they don't point to it's genuine website--with all its descriptive language, images, and subpages. Even in this case with a strong topline result, we can already get a feel for what websites will pollute our automated searches: Facebook and greatschools.org are a good start to making an exclusion list to filter the results. \n",
    "\n",
    "Now let's try something harder to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the first 10 results from Google for Dr. David C. Walker Intermediate School located at 6500 Ih 35 N Ste C, San Antonio, TX 78218. What do you notice about the results? How do they compare to the previous set of results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are much less clear and organized: Each one points to a different site, and all of them are third parties. Why would this be the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping URLs using an exclusion list<a id='exclusionlist'></a>\n",
    "\n",
    "To provide cleaner search results, let's filter out the third-party websites from the previous two examples. \n",
    "\n",
    "Many of these websites can show up with either 'http' or 'https', often with or without a 'www', but usually have a consistent top-level domain (e.g., 'com'). Exact string matchin would fail to capture matches across these variations. Regular expressions could do this, but for now let's just filter out those search results that contain the core of any domain name in the exclusion list (e.g., niche.com). \n",
    "\n",
    "Let's get the first result for the previous school (Dr. David C. Walker Intermediate School) that doesn't match any domains in the exclusion list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define excluded domains to filter out: third-party domains/false positives that we DON'T want to scrape \n",
    "exclusions = ['facebook.com', 'greatschools.org', 'niche.com', 'har.com', 'usnews.com', 'publicschoolreview.com', \n",
    "             'nces.ed.gov', 'dnb.com', 'schooldigger.com', 'elementaryschools.org', 'closelocation.com']\n",
    "\n",
    "# Define search metadata\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "#school_name = \"River City Scholars Charter Academy\"\n",
    "#school_address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "# Collect search results\n",
    "urls = search(school_name + ' ' + school_address, \\\n",
    "              stop=20, pause=5.0) # Expand search range to help avoid excluded domains\n",
    "print(\"Successfully collected Google search results.\")\n",
    "\n",
    "# Initialize exclusion list match counter: How many excluded domains has this search encountered?\n",
    "excluded_num = 0 \n",
    "\n",
    "# Loop through google search output to find first good result:\n",
    "for url in urls:\n",
    "    if any(domain in url for domain in exclusions):\n",
    "        print(f'Bad site detected: {url}') \n",
    "        excluded_num += 1 # Add one to exclusions list match counter\n",
    "    else:\n",
    "        good_url = url\n",
    "        print(\"Success! URL obtained by Google search with \" + str(excluded_num) + \" bad URLs avoided.\")\n",
    "        break # Exit for loop after first good url is found\n",
    "        \n",
    "print(f'Quality URL: {good_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of [the \"quality\" URL we landed on](https://yellow.place/en/dr-david-c-walker-int-san-antonio-tx-usa)? What does this mean about our exclusion list?\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Improve our automated searching to try to get the genuine URL of Dr. David C. Walker Intermediate School. <br/>\n",
    "_Hint_: You could try (A) adding more URLs to the exclusion list OR (B) try a simple search but for more URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
